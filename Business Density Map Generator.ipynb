{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded API stuff\n"
     ]
    }
   ],
   "source": [
    "import gmplot\n",
    "import pandas as pd\n",
    "from yelpapi import YelpAPI\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%run API_STUFF.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Scrape data from yelp from the zip codes provided and the search term.\n",
    "def search_yelp(state, term, zips):\n",
    "    #yelp API authorization and shared secret\n",
    "    yelp_api = YelpAPI(API_KEY, API_SECRET)\n",
    "    #run test search to get the center Lat and Long we need to center the map on and \n",
    "    #find total results for this zip\n",
    "    try:\n",
    "        search_results = yelp_api.search_query(term=term, location = state, limit = 1, radius_filter= 4000)\n",
    "    except:\n",
    "        search_results = yelp_api.search_query(term=term, location = state, limit = 1, radius_filter= 4000)\n",
    "    latitude = []\n",
    "    longitude = []\n",
    "    price = []\n",
    "    raiting = []\n",
    "    review_count = []\n",
    "    name = []\n",
    "    zipcode = []\n",
    "    ZIP = []\n",
    "    closed = 0\n",
    "    nozip = 0\n",
    "    #search each zip code in the array of zips from this state\n",
    "    for i, zipcode in enumerate(zips):\n",
    "        #print statement to follow progress of search\n",
    "        print('Processing zipcode {} of {}.  Zip : {}'.format(i,len(zips), zipcode), end = '\\r')\n",
    "        #sometimes yelp API throws errors, so try and if it fails try again.\n",
    "        try:\n",
    "            search_results = yelp_api.search_query(term=term, location = zipcode, limit = 1, radius_filter= 4000)\n",
    "        except:\n",
    "            search_results = yelp_api.search_query(term=term, location = zipcode, limit = 1, radius_filter= 4000)\n",
    "        #save total number of restuarants in search so we know how many pages to scrape\n",
    "        total_restaurants = search_results['total']\n",
    "        #Yelp only lets us scrap 1000 results so set the page counter accordingly, \n",
    "        #1000 OR total restaurants /50 results a page\n",
    "        if(total_restaurants<1000):\n",
    "            tosearch = total_restaurants//50\n",
    "        else:\n",
    "            tosearch = 20\n",
    "        #run the search on every yelp results query we are allowed to\n",
    "        for search in range(tosearch):\n",
    "\n",
    "            #get the results for the search 50 at a time\n",
    "            try:\n",
    "                newsearch = yelp_api.search_query(term=term, location = zipcode, limit = 50, offset = 50*search, radius_filter= 4000)\n",
    "            except:\n",
    "                newsearch = yelp_api.search_query(term=term, location = zipcode, limit = 50, offset = 50*search, radius_filter= 4000)\n",
    "            for entry in range(50):\n",
    "                #try to run the search, if yelp says no page, youve hit the end of the results so dont do anything\n",
    "                try:\n",
    "                    #if the business is open (or not closed) save the relavant information\n",
    "                    if(newsearch['businesses'][entry]['is_closed'] == False):\n",
    "                        latitude.append(newsearch['businesses'][entry]['coordinates']['latitude'])\n",
    "                        longitude.append(newsearch['businesses'][entry]['coordinates']['longitude'])\n",
    "                        name.append(newsearch['businesses'][entry]['id'])\n",
    "                        #some restaurants have no price rating and it was throwing an error, if no rating rate a 0\n",
    "                        try:\n",
    "                            price.append(newsearch['businesses'][entry]['price'])\n",
    "                        except:\n",
    "                            price.append('')\n",
    "                        raiting.append(newsearch['businesses'][entry]['rating'])\n",
    "                        review_count.append(newsearch['businesses'][entry]['review_count'])\n",
    "                        #some restaurants have no zip code saved, if this is the case save no zip, eliminate entry later\n",
    "                        try:\n",
    "                            ZIP.append(newsearch['businesses'][entry]['location']['zip_code'])\n",
    "                        except:\n",
    "                            ZIP.append('')\n",
    "                    #if entry is closed iterate closed counter just so we can know about them\n",
    "                    else:\n",
    "                        closed = closed + 1\n",
    "                except:\n",
    "                    None\n",
    "    #save all the data parsed from scraping into a dataframe\n",
    "    data = pd.DataFrame()\n",
    "    data['name'] = name\n",
    "    data['latitude'] = latitude\n",
    "    data['longitude'] = longitude\n",
    "    data['price'] = [len(x) for x in price]\n",
    "    data['raiting'] = raiting\n",
    "    data['review_count'] = review_count\n",
    "    data['Zipcode'] = ZIP\n",
    "    #print some basic info about the scrape we ran\n",
    "    print('{} businesses were closed and not included in the data'.format(closed))\n",
    "    print('{} businesses had no zip and were not included in the data'.format(nozip))\n",
    "    #return the dataframe as well as the central location that the map should be centered on\n",
    "    return data, search_results['region']['center']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pop_data(location):\n",
    "    #https://blog.splitwise.com/2013/09/18/the-2010-us-census-population-by-zip-code-totally-free/\n",
    "    #load data referencing population to zip code\n",
    "    population_by_zip = pd.read_csv('Pop_by_zip.csv', names= ['Zipcode', 'Pop'], skiprows=1)\n",
    "    #make the zip code a numeric value\n",
    "    population_by_zip.Zipcode = pd.to_numeric(population_by_zip.Zipcode)\n",
    "    #http://federalgovernmentzipcodes.us/\n",
    "    #load the data referencing the latitude and longitude of each zip code\n",
    "    zip_code_lat_long = pd.read_csv('zipcode-database.csv')\n",
    "    #keep the columns that we want to use\n",
    "    zip_code_lat_long = zip_code_lat_long[['Zipcode', 'State', 'Lat', 'Long', 'City']]\n",
    "    #keep the entries from the state we are investigating\n",
    "    zip_code_lat_long = zip_code_lat_long[(zip_code_lat_long.State == location)]\n",
    "    #make the zip code a numeric value\n",
    "    zip_code_lat_long.Zipcode = pd.to_numeric(zip_code_lat_long.Zipcode)\n",
    "    #merge the data on zip code so we know the population by latitude and longitude\n",
    "    pop_by_lat_long = zip_code_lat_long.merge(population_by_zip, on='Zipcode', how='inner')\n",
    "    #fill Nan values with 1 for zip codes that have no people stored\n",
    "    pop_by_lat_long = pop_by_lat_long.fillna(1)\n",
    "    #placeholder for list of zips we are working with\n",
    "    zips = []\n",
    "    #generate an array of the zipcodes important to this state\n",
    "    for zipcode in pop_by_lat_long.Zipcode:\n",
    "        #exclude the zip code that Yelp isn't recognizing from MI\n",
    "        if len(str(zipcode)) == 5 and (zipcode != 48921):\n",
    "            zips.append(zipcode)\n",
    "    #return the dataframe and a list of the zip codes for this state\n",
    "    return pop_by_lat_long, zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generates a list of lat and longs based on the density of businesses of the specified type per capita, the more \n",
    "#businesses the more times each lat long pair will be returned\n",
    "def reformat_pop_entries(df):\n",
    "    #create a new datafame with the appropriate headers\n",
    "    new_df = pd.DataFrame(columns = ['Lat', 'Long'])\n",
    "    #make sure the index count is sequential, used for tracking progress\n",
    "    df = df.reset_index(drop=True)\n",
    "    #placeholder array for heat value: a value that is higher when the density of businesses per capita is higher\n",
    "    heatvalue = []\n",
    "    #for each row (zipcode in the state that contains businesses of the search type), calculate businesses per capita \n",
    "    for ind, location in df.iterrows():\n",
    "        #print a progress report\n",
    "        print('On row {} of {}'.format(ind, len(df)), end = '\\r')\n",
    "        \n",
    "        #if people actually live in this zip code, calcultate the business density\n",
    "        if(location.Pop != 0):\n",
    "            heat = location['counts']/location.Pop\n",
    "        #else, assign this as nan, we can drop this later, nobody lives there\n",
    "        else:\n",
    "            heat = float('NaN')\n",
    "        #save the business density value in the list\n",
    "        heatvalue.append(heat)\n",
    "    #put together the new dataframe with our business density data\n",
    "    new_df['Lat'] = df.Lat\n",
    "    new_df['Long'] = df.Long\n",
    "    new_df['City'] = df.City\n",
    "    new_df['Pop'] = df.Pop\n",
    "    new_df['heatvalue'] = heatvalue\n",
    "    new_df = new_df.dropna()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(new_df['heatvalue'])\n",
    "    #normalize the data from 1 to 0, the place with the highest density of target business type as 100, places with \n",
    "    #no businesses of the searched type as 0\n",
    "    new_df['heatvalue'] = scaler.transform(new_df['heatvalue'].reshape(-1, 1))*100\n",
    "    \n",
    "    #dropcount = []\n",
    "    #prepare the new dataframe that will contain lat long pairs\n",
    "    latlongcounts = pd.DataFrame()\n",
    "    #for each zip code\n",
    "    for i, location in new_df.iterrows():\n",
    "        #generate between 100 and 0 rows of lat long pairs depending on the density of the business type per capita\n",
    "        for number in range(int(round(location.heatvalue))):\n",
    "            #create a single entry for the heatmap to use as a measure of density\n",
    "            temp = pd.DataFrame([[location.Lat, location.Long, location.City]], columns=['Lat', 'Long', 'City'])\n",
    "            #add the entry to the dataframe\n",
    "            latlongcounts = latlongcounts.append(temp)\n",
    "    #make sure the index is sequential (it should already be but I like doing this)\n",
    "    latlongcounts = latlongcounts.reset_index(drop=True)\n",
    "    return latlongcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(location, term):\n",
    "    #compile the list of populations by lat and long and retieve the  list of relavant zips\n",
    "    pop_by_zip, zips = load_pop_data(location)\n",
    "    #search yelp for all the entries we can find\n",
    "    restaurants, center = search_yelp(location, term, zips)\n",
    "    #initiate map data and center it on the middle of the state we are looking at\n",
    "    gmap = gmplot.GoogleMapPlotter(center['latitude'], center['longitude'], 7.5)\n",
    "    #because we are searching by zip with a range of 25 miles sometimes we will get duplicate entries \n",
    "    #drop entries will identical id values because we picked them up more than once by searching adjacent zips\n",
    "    restaurants.drop_duplicates(subset = 'name', inplace = True)\n",
    "    #set the count of each restuarant to 1 next to its entry, will be used by groupby function\n",
    "    restaurants['counts'] = 1\n",
    "    #group all the data by zip code so we know how many businesses are in each zip\n",
    "    restaurantcounts = restaurants.groupby('Zipcode', as_index=False).sum()\n",
    "    #drop the columns we aren't interested in\n",
    "    restaurantcounts = restaurantcounts[['Zipcode', 'counts']]\n",
    "    #prepare the zipcode to be merged by typing it as string\n",
    "    restaurantcounts['Zipcode'] = [str(x)  for x in restaurantcounts['Zipcode']]\n",
    "    #prepare the zipcode to be merged by typing it as string\n",
    "    pop_by_zip['Zipcode'] = [str(x)  for x in pop_by_zip['Zipcode']]\n",
    "    #merge the zipcode and population data with the restuarant counts by zip\n",
    "    pop_by_zip_w_b_counts = pop_by_zip.merge(restaurantcounts, on='Zipcode', how='outer')\n",
    "    #drop rows where there are nan values\n",
    "    pop_by_zip_w_b_counts_clean = pop_by_zip_w_b_counts.dropna()\n",
    "    #return a list of lats and longs that is generated by determining how many businesses of this type per capita\n",
    "    entries_adj_pop = reformat_pop_entries(pop_by_zip_w_b_counts_clean)\n",
    "    #plot out a heat map using lat and longs, the more businesses per capita the more times each lat long pair\n",
    "    #is in the entries_adj_pop dataframe\n",
    "    gmap.heatmap(entries_adj_pop.Lat, entries_adj_pop.Long, radius = 40)\n",
    "    #prepair the filename based of search terms\n",
    "    filename = \"Maps/\" +term + '_in_' + location + \".html\"\n",
    "    #draw the file\n",
    "    gmap.draw(filename)\n",
    "    print('Map file saved as {}'.format(filename))\n",
    "    #return the dataframe that contains the counts of restaurants in each zip/city for inspection if needed\n",
    "    return pop_by_zip_w_b_counts_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 businesses were closed and not included in the data\n",
      "0 businesses had no zip and were not included in the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:324: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map file saved as Maps/Pizza_in_MI.html\n"
     ]
    }
   ],
   "source": [
    "allstuff = run(location = 'MI',term =  'Pizza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allstuff = run(location = 'CA',term =  'Pizza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "allstuff = run(location = 'PA',term =  'Chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allstuff = run(location = 'PA',term =  'PIZZA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
